{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91be646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Creation des modÃ©les afin de de pouvoir rajouter la Batch Normalization\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,n_state,n_action,act_limit,layers=[30,30],activation=nn.ReLU,finalActivation=None,dropout=0.0):\n",
    "        super(Actor,self).__init__()\n",
    "        self.n_state=n_state\n",
    "        self.n_action=n_action\n",
    "        self.act_limit=act_limit\n",
    "        ##################################################\n",
    "        layer = nn.ModuleList([])\n",
    "        inSize =n_state\n",
    "        for x in layers:\n",
    "            layer.append(nn.Linear(inSize, x))\n",
    "            layer.append(activation())\n",
    "            if dropout > 0:\n",
    "                layer.append(nn.Dropout(dropout)) \n",
    "            inSize=x  \n",
    "        layer.append(nn.Linear(inSize, n_action))\n",
    "        layer.append(nn.Tanh())\n",
    "        if finalActivation:\n",
    "            layer.append(finalActivation())\n",
    "        self.actor=nn.Sequential(*layer)\n",
    "        ##################################################\n",
    "\n",
    "\n",
    "    def forward(self,obs):\n",
    "        return self.act_limit*self.actor(obs)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,n_state,n_action,layers=[30,30],activation=nn.ReLU,finalActivation=None,dropout=0.0,use_batch_norm=False):\n",
    "        super(Critic,self).__init__()\n",
    "        self.n_state=n_state\n",
    "        self.n_action=n_action\n",
    "        layer = nn.ModuleList([])\n",
    "        inSize =n_state+n_action\n",
    "        for x in layers:\n",
    "            layer.append(nn.Linear(inSize, x))\n",
    "            if use_batch_norm:\n",
    "                layer.append(nn.BatchNorm1d(num_features=x))\n",
    "            layer.append(activation())\n",
    "            if dropout > 0:\n",
    "                layer.append(nn.Dropout(dropout)) \n",
    "            inSize = x\n",
    "        layer.append(nn.Linear(inSize, 1))\n",
    "        if finalActivation:\n",
    "            layer.append(finalActivation())\n",
    "        self.critic=nn.Sequential(*layer)\n",
    "    \n",
    "    def forward(self,obs,action):\n",
    "        return self.critic(torch.cat([obs,action],dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cde57b65",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Critic' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-3f8023802f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinalActivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcritic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(actor.actor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-800d48d00697>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_state, n_action, layers, activation, finalActivation, dropout, use_batch_norm)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfinalActivation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinalActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Critic' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    actor=Actor(8,2,act_limit=2,layers=[30,30],activation=nn.LeakyReLU,finalActivation=nn.Tanh)\n",
    "    critic=Critic(8,2,layers=[30,30],activation=nn.LeakyReLU)\n",
    "    #print(actor.actor)\n",
    "    print(actor)\n",
    "    print(critic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
