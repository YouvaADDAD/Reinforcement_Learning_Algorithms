{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a4787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "from randomAgent import RandomAgent\n",
    "\n",
    "\n",
    "class PolicyIteration(RandomAgent):\n",
    "    def __init__(self, env,epsilon=1e-3,gamma=0.99):\n",
    "        super(PolicyIteration,self).__init__(env.action_space)\n",
    "        self.env = env\n",
    "        self.epsilon=epsilon\n",
    "        self.gamma=gamma\n",
    "        self.states,self.mdp=env.getMDP()\n",
    "        self.n_action=self.env.action_space.n\n",
    "        self.actions=self.env.action_space\n",
    "        self.n_states=len(self.states)\n",
    "        self.policy = np.array([self.actions.sample() for _ in range(self.n_states)])\n",
    "        self.value  = np.zeros(self.n_states)\n",
    "\n",
    "    \n",
    "    def act(self, observation):\n",
    "         return self.policy[observation]\n",
    "\n",
    "    def learn(self):\n",
    "        while True :\n",
    "            value_new = np.zeros(self.n_states)\n",
    "            for s in self.mdp.keys():\n",
    "                action=self.policy[s]\n",
    "                value_new[s]=np.sum([proba * (reward+self.gamma*self.value[next_state])  for proba,next_state,reward,_ in  self.mdp[s].get(action)])     \n",
    "            if np.linalg.norm(self.value-value_new)<=self.epsilon:\n",
    "                break\n",
    "            self.value=copy.deepcopy(value_new)\n",
    "        \n",
    "        policy_new=np.zeros(self.n_states,dtype=int)\n",
    "        for s in self.mdp.keys():\n",
    "            policy_new[s]=np.argmax([np.sum([proba_trans * (reward+self.gamma*self.value[s_prime])  for proba_trans,s_prime,reward,_ in self.mdp[s].get(a)]) for a in range(self.n_action)])\n",
    "\n",
    "        condition= np.all(policy_new ==self.policy) \n",
    "        self.policy=copy.deepcopy(policy_new)\n",
    "        return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4036269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "(array([[1, 1, 1, 1, 1, 1],\n",
      "       [1, 0, 0, 0, 3, 1],\n",
      "       [1, 0, 1, 0, 2, 1],\n",
      "       [1, 0, 0, 0, 0, 1],\n",
      "       [1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1]]), -1, True, {})\n",
      "  (North)\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[42m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[44m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "  (North)\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[42m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[44m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[47m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\u001b[40m \u001b[0m\n",
      "Nombre d'etats :  11\n",
      "0\n",
      "{0: [(0.8, 0, -0.001, False), (0.1, 0, -0.001, False), (0.1, 2, -0.001, False)], 1: [(0.8, 1, -1, True), (0.1, 0, -0.001, False), (0.1, 2, -0.001, False)], 2: [(0.1, 0, -0.001, False), (0.1, 1, -1, True), (0.8, 2, -0.001, False)], 3: [(0.1, 0, -0.001, False), (0.1, 1, -1, True), (0.8, 0, -0.001, False)]}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    env = gym.make('gridworld-v0')\n",
    "    env.setPlan(\"gridworldPlans/plan0.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "    env.reset()\n",
    "    \n",
    "    env.seed(0)  # Initialise le seed du pseudo-random\n",
    "    print(env.action_space)  # Quelles sont les actions possibles\n",
    "    print(env.step(1))  # faire action 1 et retourne l'observation, le reward, et un done un booleen (jeu fini ou pas)\n",
    "    env.render()  # permet de visualiser la grille du jeu\n",
    "    env.render(mode=\"human\") #visualisation sur la console\n",
    "    states, mdp = env.getMDP()  # recupere le mdp et la liste d'etats\n",
    "    print(\"Nombre d'etats : \",len(states))\n",
    "    state, transitions = list(mdp.items())[0]\n",
    "    print(state)  # un etat du mdp\n",
    "    print(transitions)  # dictionnaire des transitions pour l'etat :  {action-> [proba,etat,reward,done]}\n",
    "    # Execution avec un Agent\n",
    "    agent = PolicyIteration(env)\n",
    "\n",
    "    episode_count = 1000\n",
    "    reward = 0\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    i=0\n",
    "    condition= True\n",
    "    obs = env.reset()\n",
    "    while condition:\n",
    "        print('ok')\n",
    "        env.verbose = (i % 100 == 0 and i > 0)  # afficher 1 episode sur 100\n",
    "        if env.verbose:\n",
    "            env.render()\n",
    "        j = 0\n",
    "        rsum = 0\n",
    "        action = agent.act(env.getStateFromObs(obs))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        condition=agent.learn()\n",
    "        state = next_state\n",
    "        rsum += reward\n",
    "        j += 1\n",
    "        if env.verbose:\n",
    "                env.render()\n",
    "        if done or condition:\n",
    "                print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
    "                break\n",
    "\n",
    "    print(\"done\")\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
